Building a Distributed RDBMS using Raft

dongxu
CTO, PingCAP
huang@pingcap.com
http://github.com/pingcap

* Introduction

* Wait... Why not NoSQL?

- No more workarounds, please
- No! SQL

* Why another RDBMS?

What's wrong? What we need?

- Scalability
- ACID Transaction
- SQL Support
- Auto failover

*3S*: *Scalable* / *Survivable* / *SQL*

* What is Raft? Why Raft?

- designed to be easy to understand
- equivalent to Paxos in *fault-tolerance* and performance
- includes a simple cluster *membership* change mechanism

Not like Paxos, Raft is a *Replicated* *State* *Machine*

* RSM vs Consensus Algorithm

Paxos (Consensus Algorithm)

- Agree on one *single* value in a distributed environment

Raft / Multi Paxos (RSM)

- Agree on the same *sequence* of values

Paxos is simpler than Replicated State Machine

* Replication Model

* 1-Master / N-Slave

Traditional RDBMS, e.g. MySQL, PostgreSQL ...

*Advantage*

- Easy to understand, easy to implement

*Disadvantage*

- SPOF, low reliability
- Failover / Recovery is a nightmare

And if you want a strong consistency guarantee, the latency will become unacceptable.

* Gossip

widely used for service / node discovery, or share configration over a large network.

*Advantange*

- Pure P2P model, easy to deploy

*Disadvantage*

- You would never know when all the nodes reach a consensus, orz...
- Implementation complexity, very hard to debug.

* Quorum Model (Paxos, Raft)

*Advantange*

- Auto failover
- Acceptable latency
- WRN, flexable consistency model

*Disadvantage*

- Implementation complexity

* How It Works

* Leader election (1/3)

At any given time, each node is either:

- *Leader*: handles all client operations, log replication
- *Follower*: only respond to incoming RPCs
- *Candidate*: wait for votes, new leader candidate

At most 1 viable leader at a time!

* Leader election (2/3)

.image imgs/raft_fsm.png _ 1000

* Leader election (3/3)

- Term
- Vote split

* Log

Log := {term, index, command}
Each state machine processes the same series of commands thus arrives at the same series of states.

.image imgs/log.png _ 550

* Log replication

The leader decides when it is safe to apply a log to the state machines

- once a log has replicated it on a majority of the servers
- such a log is called *committed*

* Log replication (1/3)

.image imgs/log_replicate.png _ 1000

Example of a simple, failure free commit: node 1 commits y ← 9 in term 1 at index 3.

* Log replication - Safty (2/3)

Target: New leader always has the newest log

    // On vote request
    if A.maxTerm > B.maxTerm {
        return A
    } else if A.maxTerm < B.maxTerm {
        return B
    } else {
        return A.maxIndex > B.maxIndex ? A : B
    }

* Log replication (3/3)

TBD

* Config change

TBD

* RDBMS & Raft

* Raft vs Multi Raft

*Raft* ( Etcd / Consul / ... )

- Single RSM

*Multi-Raft* ( CockroachDB / TiKV / ... )

- Muiltiple RSM

* TiDB Architecture 

.image imgs/tidb_stack.png _ 800

* TiKV Architecture

.image imgs/tikv_stack.png _ 1000

* Region split / merge

TBD

* Metadata manangment 

Region lookup

-TiClient
-Placement Driver
-Etcd


* MVCC

TBD

* Transaction Model (1/2)

* Transaction Model (2/2)

* GC

TBD

* MPP Query

TBD

