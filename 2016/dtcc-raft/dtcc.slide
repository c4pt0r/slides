Building a Distributed RDBMS using Raft

dongxu
CTO, PingCAP
huang@pingcap.com
http://github.com/pingcap

* Introduction

* Wait... Why not NoSQL?

- No more workarounds, please
- No! SQL

* Why another RDBMS?

What's wrong? What we need?

- Scalability
- ACID Transaction
- SQL Support
- Auto failover

*3S*: *Scalable* / *Survivable* / *SQL*

* What is Raft? Why Raft?

- designed to be easy to understand
- equivalent to Paxos in *fault-tolerance* and performance
- includes a simple cluster *membership* change mechanism

Not like Paxos, Raft is a *Replicated* *State* *Machine*

* RSM vs Consensus Algorithm

Paxos (Consensus Algorithm)

- Agree on one *single* value in a distributed environment

Raft / Multi Paxos (RSM)

- Agree on the same *sequence* of values

Paxos is simpler than Replicated State Machine

* Replication Model

* 1-Master / N-Slave

Traditional RDBMS, e.g. MySQL, PostgreSQL ...

*Advantage*

- Easy to understand, easy to implement

*Disadvantage*

- SPOF, low reliability
- Failover / Recovery is a nightmare

And if you want a strong consistency guarantee, the latency will become unacceptable.

* Gossip

Widely used for service / node discovery, or share configration over a large network.

*Advantange*

- Pure P2P model, easy to deploy

*Disadvantage*

- You would never know when all the nodes reach a consensus, orz...
- Implementation complexity, very hard to debug.

* Quorum Model (Paxos, Raft)

*Advantange*

- Auto failover
- Acceptable latency
- WRN, flexable consistency model

*Disadvantage*

- Implementation complexity

* How It Works

* Leader election (1/3)

At any given time, each node is either:

- *Leader*: handles all client operations, log replication
- *Follower*: only respond to incoming RPCs
- *Candidate*: wait for votes, new leader candidate

At most 1 viable leader at a time!

* Leader election (2/3)

.image imgs/raft_fsm.png _ 1000

* Leader election (3/3)

- Term
- Vote split

* Log

Log := {term, index, command}
Each state machine processes the same series of commands thus arrives at the same series of states.

.image imgs/log.png _ 550

* Log replication

The leader decides when it is safe to apply a log to the state machines

- once a log has replicated it on a majority of the servers
- such a log is called *committed*

* Log replication (1/3)

.image imgs/log_replicate.png _ 1000

Example of a simple, failure free commit: node 1 commits y ← 9 in term 1 at index 3.

* Log replication - Safty (2/3)

Target: New leader always has the newest log

    // On vote request
    if A.maxTerm > B.maxTerm {
        return A
    } else if A.maxTerm < B.maxTerm {
        return B
    } else {
        return A.maxIndex > B.maxIndex ? A : B
    }

* Log replication (3/3)

Snapshot

* Configration change (1/2)

Problem: Disjoint majorities

.image imgs/disjoint.png _ 600

* Configration change (2/2)

Solve: Single server membership change

.image imgs/conf_change.png _ 600

* RDBMS & Raft

* Raft vs Multi Raft

*Raft* ( Etcd / Consul / ... )

- Single RSM

*Multi-Raft* ( CockroachDB / TiKV / ... )

- Muiltiple RSM

* TiDB Architecture 

.image imgs/tidb_stack.png _ 800

* TiKV Architecture

.image imgs/tikv_stack.png _ 1000

* TiKV Architecture (Logical)

.image imgs/tikv_stack2.png _ 500

* Ti Project Overview

.image imgs/overview.png _ 900

* Region split / merge

.image imgs/split1.png _ 500

* Region split / merge

.image imgs/split2.png _ 650

* Region split / merge

.image imgs/split3.png _ 500

* Region split / merge

.image imgs/split4.png _ 750

* Region split / merge

.image imgs/split5.png _ 600

* Metadata manangment 

Region lookup

- TiClient
- Placement Driver
- Etcd

* MVCC

Take the advantage of LSM-Tree (Sorted Table)

 Key Layout:
 MetaKey(Key+0)              -->     Meta0
 MetaKey(Key+1)              -->     Meta1
 MetaKey(Key+2)              -->     Meta2
 DataKey(Key+'startTs1')     -->     Value_V1
 DataKey(Key+'startTs2')     -->     Value_V2
 ...
 
 Meta:
 message MetaItem {
     optional uint64 start_ts   = 1;
     optional uint64 commit_ts  = 2;
     optional bool   is_primary = 3; // for gc
 }

* Transaction Model (1/2)

- 2 Phrase Lock / 2 Phrase Commit
- Based on Percolator (Google, USENIX 2010) with some optimization
- Distributed transaction manage (binding with data)

* Transaction Model (2/2)

- Latency vs Throughput
- Lock cleaning

* GC

Target: Clean useless data, unlock out-dated locks

Biggest Problem: Suspensive locks.

- Effcient
- Correctness

* MPP Query

- Predicate/Projection/Aggregation Pushdown (to Coprocessor)
DNF->CNF, Example: A JOIN B WHERE A.id > 10 or (A.id < 5 and b.id < 5)

- Cost Modal
CPU / IO / Communication Cost / Number of rows in tables / Histogram ...


